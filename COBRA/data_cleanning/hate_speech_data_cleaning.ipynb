{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from string import punctuation\n",
    "import os\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('labeled_data.csv', sep=',',index_col=0)\n",
    "tweets=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "tweets=data['tweet']\n",
    "table = str.maketrans('', '', punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "cleandata = []\n",
    "\n",
    "for tweet in tweets[:2000]:\n",
    "    tokens = tweet.split()\n",
    "    cleanstring = []\n",
    "    for o in tokens:\n",
    "        # Clearn words that start as @\n",
    "        if o.startswith('@'): continue\n",
    "        # remove punctuation from each token\n",
    "        o = o.translate(table)  \n",
    "        # filter out short tokens \n",
    "        if len(o) < 2: continue\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        if not o.isalpha(): continue\n",
    "        # filter out stop words\n",
    "        if o in stop_words: continue\n",
    "        o = lemma.lemmatize(o)\n",
    "        cleanstring.append(o)\n",
    "    cleandata.append(cleanstring)    \n",
    "#print (cleandata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
